---
- hosts: all
  become: no
  any_errors_fatal: true
  gather_facts: no
  vars_files:
      - vars.yaml

  # https://medium.com/codex/running-a-multi-node-hadoop-cluster-257068e5f276
  # https://medium.com/codex/setting-up-a-multi-node-apache-spark-cluster-on-apache-hadoop-and-apache-hive-412764ab6881
  tasks:
    - name: Download Spark {{hadoop_version}}
      delegate_to: 127.0.0.1
      run_once: yes
      ansible.builtin.get_url:
        url: https://archive.apache.org/dist/spark/spark-{{spark_version}}/spark-{{spark_version}}-bin-hadoop3.tgz
        dest: /tmp/spark-{{spark_version}}-bin-hadoop3.tgz

    - name: Deploy Spark bundle to remote nodes
      copy:
        src: /tmp/spark-{{spark_version}}-bin-hadoop3.tgz
        dest: "{{ default_install_dir }}/spark-{{spark_version}}-bin-hadoop3.tgz"
        # checksum: "sha256:{{spark_version_sha256}}"

    - name: Unarchive Spark
      ansible.builtin.unarchive:
        src: "{{ default_install_dir }}/spark-{{spark_version}}-bin-hadoop3.tgz"
        dest: "{{ default_install_dir }}"
        remote_src: yes

    - name: Create Spark data dir
      file:
        path: "{{ item }}"
        state: directory
        mode: "g+rw"
      with_items:
        - "{{ spark_local_dirs }}"
        - "{{ java_local_dirs }}"
